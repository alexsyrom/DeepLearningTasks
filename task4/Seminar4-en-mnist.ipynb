{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Theano, Lasagne\n",
    "and why they matter\n",
    "\n",
    "\n",
    "### got no lasagne?\n",
    "Install the __bleeding edge__ version from here: http://lasagne.readthedocs.org/en/latest/user/installation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Warming up\n",
    "* Implement a function that computes the sum of squares of numbers from 0 to N\n",
    "* Use numpy or python\n",
    "* An array of numbers 0 to N - numpy.arange(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sum_squares(N):\n",
    "    return (np.arange(N) ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 308 ms, sys: 404 ms, total: 712 ms\n",
      "Wall time: 709 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "662921401752298880"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sum_squares(10**8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# theano teaser\n",
    "\n",
    "Doing the very same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#I gonna be function parameter\n",
    "N = T.scalar(\"a dimension\",dtype='int32')\n",
    "\n",
    "\n",
    "#i am a recipe on how to produce sum of squares of arange of N given N\n",
    "result = (T.arange(N)**2).sum()\n",
    "\n",
    "#Compiling the recipe of computing \"result\" given N\n",
    "sum_function = theano.function(inputs = [N],outputs=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 348 ms, sys: 168 ms, total: 516 ms\n",
      "Wall time: 406 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(662921401752298880)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sum_function(10**8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# How does it work?\n",
    "* 1 You define inputs f your future function;\n",
    "* 2 You write a recipe for some transformation of inputs;\n",
    "* 3 You compile it;\n",
    "* You have just got a function!\n",
    "* The gobbledegooky version: you define a function as symbolic computation graph.\n",
    "\n",
    "\n",
    "* There are two main kinвs of entities: \"Inputs\" and \"Transformations\"\n",
    "* Both can be numbers, vectors, matrices, tensors, etc.\n",
    "* Both can be integers, floats of booleans (uint8) of various size.\n",
    "\n",
    "\n",
    "* An input is a placeholder for function parameters.\n",
    " * N from example above\n",
    "\n",
    "\n",
    "* Transformations are the recipes for computing something given inputs and transformation\n",
    " * (T.arange(N)^2).sum() are 3 sequential transformations of N\n",
    " * Doubles all functions of numpy vector syntax\n",
    " * You can almost always go with replacing \"np.function\" with \"T.function\" aka \"theano.tensor.function\"\n",
    "   * np.mean -> T.mean\n",
    "   * np.arange -> T.arange\n",
    "   * np.cumsum -> T.cumsum\n",
    "   * and so on.\n",
    "   * builtin operations also work that way\n",
    "   * np.arange(10).mean() -> T.arange(10).mean()\n",
    "   * Once upon a blue moon the functions have different names or locations (e.g. T.extra_ops)\n",
    "     * Ask us or google it\n",
    " \n",
    " \n",
    "Still confused? We gonna fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Inputs\n",
    "example_input_integer = T.scalar(\"scalar input\",dtype='float32')\n",
    "\n",
    "example_input_tensor = T.tensor4(\"four dimensional tensor input\") #dtype = theano.config.floatX by default\n",
    "#не бойся, тензор нам не пригодится\n",
    "\n",
    "\n",
    "\n",
    "input_vector = T.vector(\"my vector\", dtype='int32') # vector of integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Transformations\n",
    "\n",
    "#transofrmation: elementwise multiplication\n",
    "double_the_vector = input_vector*2\n",
    "\n",
    "#elementwise cosine\n",
    "elementwise_cosine = T.cos(input_vector)\n",
    "\n",
    "#difference between squared vector and vector itself\n",
    "vector_squares = input_vector**2 - input_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Practice time:\n",
    "#create two vectors of size float32\n",
    "my_vector = T.vector(\"my vector\", dtype='float32')\n",
    "my_vector2 = T.vector(\"my vector 2\", dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Write a transformation(recipe):\n",
    "#(vec1)*(vec2) / (sin(vec1) +1)\n",
    "my_transformation = my_vector * my_vector2 / (T.sin(my_vector) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elemwise{true_div,no_inplace}.0\n"
     ]
    }
   ],
   "source": [
    "print( my_transformation)\n",
    "#it's okay it aint a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elemwise{true_div,no_inplace} [id A] ''   \n",
      " |Elemwise{mul,no_inplace} [id B] ''   \n",
      " | |my vector [id C]\n",
      " | |my vector 2 [id D]\n",
      " |Elemwise{add,no_inplace} [id E] ''   \n",
      "   |Elemwise{sin,no_inplace} [id F] ''   \n",
      "   | |my vector [id C]\n",
      "   |InplaceDimShuffle{x} [id G] ''   \n",
      "     |TensorConstant{1} [id H]\n"
     ]
    }
   ],
   "source": [
    "#What's inside the transformation\n",
    "theano.printing.debugprint(my_transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Compiling\n",
    "* So far we were using \"symbolic\" variables and transformations\n",
    " * Defining the recipe for computation, but not computing anything\n",
    "* To use the recipe, one should compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inputs = [my_vector, my_vector2]\n",
    "outputs = [my_transformation]\n",
    "\n",
    "# The next lines compile a function that takes two vectors and computes your transformation\n",
    "my_function = theano.function(\n",
    "    inputs,outputs,\n",
    "    allow_input_downcast=True #automatic type casting for input parameters (e.g. float64 -> float32)\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using python lists:\n",
      "[array([  2.1721766 ,   5.23752832,  15.77397728], dtype=float32)]\n",
      "\n",
      "using numpy arrays:\n",
      "[array([   0.        ,    2.77555895,    5.47030783,   14.02131271,\n",
      "         89.5477066 ,  676.25805664,   47.183918  ,   24.4084301 ,\n",
      "         23.68156242,   38.24041748], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#using function with, lists:\n",
    "print(\"using python lists:\")\n",
    "print(my_function([1,2,3],[4,5,6]))\n",
    "print()\n",
    "\n",
    "#Or using numpy arrays:\n",
    "#btw, that 'float' dtype is casted to secong parameter dtype which is float32\n",
    "print(\"using numpy arrays:\")\n",
    "print(my_function(np.arange(10),\n",
    "                  np.linspace(5,6,10,dtype='float')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Debugging\n",
    "* Compilation can take a while for big functions\n",
    "* To avoid waiting, one can evaluate transformations without compiling\n",
    "* Without compilation, the code runs slower, so consider reducing input size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.1721766    5.23752832  15.77397728]\n",
      "add 2 vectors [ 5.  7.  9.]\n",
      "vector's shape: [3]\n"
     ]
    }
   ],
   "source": [
    "#a dictionary of inputs\n",
    "my_function_inputs = {\n",
    "    my_vector:[1,2,3],\n",
    "    my_vector2:[4,5,6]\n",
    "}\n",
    "\n",
    "# evaluate my_transformation\n",
    "# has to match with compiled function output\n",
    "print(my_transformation.eval(my_function_inputs))\n",
    "\n",
    "\n",
    "# can compute transformations on the fly\n",
    "print(\"add 2 vectors\", (my_vector + my_vector2).eval(my_function_inputs))\n",
    "\n",
    "#!WARNING! if your transformation only depends on some inputs,\n",
    "#do not provide the rest of them\n",
    "print(\"vector's shape:\", my_vector.shape.eval({\n",
    "        my_vector:[1,2,3]\n",
    "    }))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* When debugging, it's usually a good idea to reduce the scale of your computation. E.g. if you train on batches of 128 objects, debug on 2-3.\n",
    "* If it's imperative that you run a large batch of data, consider compiling with mode='debug' instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Your turn: Mean Squared Error (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Quest #1 - implement a function that computes a mean squared error of two input vectors\n",
    "# Your function has to take 2 vectors and return a single number\n",
    "\n",
    "truth = T.vector('truth', dtype='float32')\n",
    "tested = T.vector('tested', dtype='float32')\n",
    "mse_transformation = ((truth - tested) ** 2).mean()\n",
    "\n",
    "compute_mse = theano.function(inputs=[truth, tested], outputs=[mse_transformation], allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "for n in [1,5,10,10**3]:\n",
    "    \n",
    "    elems = [np.arange(n),np.arange(n,0,-1), np.zeros(n),\n",
    "             np.ones(n),np.random.random(n),np.random.randint(100,size=n)]\n",
    "    \n",
    "    for el in elems:\n",
    "        for el_2 in elems:\n",
    "            true_mse = np.array(mean_squared_error(el,el_2))\n",
    "            my_mse = compute_mse(el,el_2)\n",
    "            if not np.allclose(true_mse,my_mse):\n",
    "                print ('Wrong result:')\n",
    "                print ('mse(%s,%s)'%(el,el_2))\n",
    "                print (\"should be: %f, but your function returned %f\"%(true_mse,my_mse))\n",
    "                raise ValueError(\"Что-то не так\")\n",
    "\n",
    "print (\"All tests passed\")\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Shared variables\n",
    "\n",
    "* The inputs and transformations only exist when function is called\n",
    "\n",
    "* Shared variables always stay in memory like global variables\n",
    " * Shared variables can be included into a symbolic graph\n",
    " * They can be set and evaluated using special methods\n",
    "   * but they can't change value arbitrarily during symbolic graph computation\n",
    "   * we'll cover that later;\n",
    " \n",
    " \n",
    "* Hint: such variables are a perfect place to store network parameters\n",
    " * e.g. weights or some metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#creating shared variable\n",
    "shared_vector_1 = theano.shared(np.ones(10,dtype='float64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial value [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#evaluating shared variable (outside symbolicd graph)\n",
    "print (\"initial value\",shared_vector_1.get_value())\n",
    "\n",
    "# within symbolic graph you use them just as any other inout or transformation, not \"get value\" needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new value [ 0.  1.  2.  3.  4.]\n"
     ]
    }
   ],
   "source": [
    "#setting new value\n",
    "shared_vector_1.set_value( np.arange(5) )\n",
    "\n",
    "#getting that new value\n",
    "print (\"new value\", shared_vector_1.get_value())\n",
    "\n",
    "#Note that the vector changed shape\n",
    "#This is entirely allowed... unless your graph is hard-wired to work with some fixed shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Your turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write a recipe (transformation) that computes an elementwise transformation of shared_vector and input_scalar\n",
    "#Compile as a function of input_scalar\n",
    "\n",
    "input_scalar = T.scalar('coefficient',dtype='float32')\n",
    "\n",
    "scalar_times_shared = input_scalar * shared_vector_1\n",
    "\n",
    "\n",
    "shared_times_n = theano.function(inputs=[input_scalar], outputs=[scalar_times_shared], allow_input_downcast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared: [ 0.  1.  2.  3.  4.]\n",
      "shared_times_n(5) [array([  0.,   5.,  10.,  15.,  20.])]\n",
      "shared_times_n(-0.5) [array([-0. , -0.5, -1. , -1.5, -2. ])]\n"
     ]
    }
   ],
   "source": [
    "print(\"shared:\", shared_vector_1.get_value())\n",
    "\n",
    "print (\"shared_times_n(5)\",shared_times_n(5))\n",
    "\n",
    "print (\"shared_times_n(-0.5)\",shared_times_n(-0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared: [-1.  0.  1.]\n",
      "shared_times_n(5) [array([-5.,  0.,  5.])]\n",
      "shared_times_n(-0.5) [array([ 0.5, -0. , -0.5])]\n"
     ]
    }
   ],
   "source": [
    "#Changing value of vector 1 (output should change)\n",
    "shared_vector_1.set_value([-1,0,1])\n",
    "print (\"shared:\", shared_vector_1.get_value())\n",
    "\n",
    "print (\"shared_times_n(5)\",shared_times_n(5))\n",
    "\n",
    "print (\"shared_times_n(-0.5)\",shared_times_n(-0.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# T.grad - why theano matters\n",
    "* Theano can compute derivatives and gradients automatically\n",
    "* Derivatives are computed symbolically, not numerically\n",
    "\n",
    "Limitations:\n",
    "* You can only compute a gradient of a __scalar__ transformation over one or several scalar or vector (or tensor) transformations or inputs.\n",
    "* A transformation has to have float32 or float64 dtype throughout the whole computation graph\n",
    " * derivative over an integer has no mathematical sense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "my_scalar = T.scalar(name='input',dtype='float64')\n",
    "\n",
    "scalar_squared = T.sum(my_scalar**2)\n",
    "\n",
    "#a derivative of v_squared by my_vector\n",
    "derivative = T.grad(scalar_squared,my_scalar)\n",
    "\n",
    "fun = theano.function([my_scalar],scalar_squared)\n",
    "grad = theano.function([my_scalar],derivative) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f0c1289e278>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIYRwMRLsQgjhYiTYhRDCxUiwCyGEi/k/xvSbsc3+s30AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0c12bc8c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "x = np.linspace(-3,3)\n",
    "x_squared = list(map(fun,x))\n",
    "x_squared_der = list(map(grad,x))\n",
    "\n",
    "plt.plot(x, x_squared,label=\"x^2\")\n",
    "plt.plot(x, x_squared_der, label=\"derivative\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Why that rocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "my_vector = T.vector('float64')\n",
    "\n",
    "#Compute the gradient of the next weird function over my_scalar and my_vector\n",
    "#warning! Trying to understand the meaning of that function may result in permanent brain damage\n",
    "\n",
    "weird_psychotic_function = ((my_vector+my_scalar)**(1+T.var(my_vector)) +1./T.arcsinh(my_scalar)).mean()/(my_scalar**2 +1) + 0.01*T.sin(2*my_scalar**1.5)*(T.sum(my_vector)* my_scalar**2)*T.exp((my_scalar-4)**2)/(1+T.exp((my_scalar-4)**2))*(1.-(T.exp(-(my_scalar-4)**2))/(1+T.exp(-(my_scalar-4)**2)))**2\n",
    "\n",
    "\n",
    "der_by_scalar,der_by_vector = T.grad(weird_psychotic_function, my_scalar), T.grad(weird_psychotic_function, my_vector)\n",
    "\n",
    "\n",
    "compute_weird_function = theano.function([my_scalar,my_vector],weird_psychotic_function)\n",
    "compute_der_by_scalar = theano.function([my_scalar,my_vector],der_by_scalar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f0c09d59240>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIQlfCCF6CUn4QgjRS/x/N7nGiBT0/xsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0c152a66a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting your derivative\n",
    "vector_0 = [1,2,3]\n",
    "\n",
    "scalar_space = np.linspace(0,7)\n",
    "\n",
    "y = [compute_weird_function(x,vector_0) for x in scalar_space]\n",
    "plt.plot(scalar_space,y,label='function')\n",
    "y_der_by_scalar = [compute_der_by_scalar(x,vector_0) for x in scalar_space]\n",
    "plt.plot(scalar_space,y_der_by_scalar,label='derivative')\n",
    "plt.grid();plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Almost done - Updates\n",
    "\n",
    "* updates are a way of changing shared variables at after function call.\n",
    "\n",
    "* technically it's a dictionary {shared_variable : a recipe for new value} which is has to be provided when function is compiled\n",
    "\n",
    "That's how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Multiply shared vector by a number and save the product back into shared vector\n",
    "\n",
    "inputs = [input_scalar]\n",
    "outputs = [scalar_times_shared] #return vector times scalar\n",
    "\n",
    "my_updates = {\n",
    "    shared_vector_1:scalar_times_shared #and write this same result bach into shared_vector_1\n",
    "}\n",
    "\n",
    "compute_and_save = theano.function(inputs, outputs, updates=my_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shared value: [ 0.  1.  2.  3.  4.]\n",
      "compute_and_save(2) returns [array([ 0.,  2.,  4.,  6.,  8.])]\n",
      "new shared value: [ 0.  2.  4.  6.  8.]\n"
     ]
    }
   ],
   "source": [
    "shared_vector_1.set_value(np.arange(5))\n",
    "\n",
    "#initial shared_vector_1\n",
    "print (\"initial shared value:\" ,shared_vector_1.get_value())\n",
    "\n",
    "# evaluating the function (shared_vector_1 will be changed)\n",
    "print (\"compute_and_save(2) returns\",compute_and_save(2))\n",
    "\n",
    "#evaluate new shared_vector_1\n",
    "print (\"new shared value:\" ,shared_vector_1.get_value())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Logistic regression example (4 pts)\n",
    "\n",
    "Implement the regular logistic regression training algorithm\n",
    "\n",
    "Tips:\n",
    "* Weights fit in as a shared variable\n",
    "* X and y are potential inputs\n",
    "* Compile 2 functions:\n",
    " * train_function(X,y) - returns error and computes weights' new values __(through updates)__\n",
    " * predict_fun(X) - just computes probabilities (\"y\") given data\n",
    " \n",
    " \n",
    "We shall train on a two-class MNIST dataset\n",
    "* please note that target y are {0,1} and not {-1,1} as in some formulae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y [shape - (360,)]: [0 1 0 1 0 1 0 0 1 1]\n",
      "X [shape - (360, 64)]:\n",
      "[[  0.   0.   5.  13.   9.   1.   0.   0.   0.   0.  13.  15.  10.  15.\n",
      "    5.   0.   0.   3.  15.   2.   0.  11.   8.   0.   0.   4.  12.   0.\n",
      "    0.   8.   8.   0.   0.   5.   8.   0.   0.   9.   8.   0.   0.   4.\n",
      "   11.   0.   1.  12.   7.   0.   0.   2.  14.   5.  10.  12.   0.   0.\n",
      "    0.   0.   6.  13.  10.   0.   0.   0.]\n",
      " [  0.   0.   0.  12.  13.   5.   0.   0.   0.   0.   0.  11.  16.   9.\n",
      "    0.   0.   0.   0.   3.  15.  16.   6.   0.   0.   0.   7.  15.  16.\n",
      "   16.   2.   0.   0.   0.   0.   1.  16.  16.   3.   0.   0.   0.   0.\n",
      "    1.  16.  16.   6.   0.   0.   0.   0.   1.  16.  16.   6.   0.   0.\n",
      "    0.   0.   0.  11.  16.  10.   0.   0.]\n",
      " [  0.   0.   1.   9.  15.  11.   0.   0.   0.   0.  11.  16.   8.  14.\n",
      "    6.   0.   0.   2.  16.  10.   0.   9.   9.   0.   0.   1.  16.   4.\n",
      "    0.   8.   8.   0.   0.   4.  16.   4.   0.   8.   8.   0.   0.   1.\n",
      "   16.   5.   1.  11.   3.   0.   0.   0.  12.  12.  10.  10.   0.   0.\n",
      "    0.   0.   1.  10.  13.   3.   0.   0.]]\n",
      "[0 1 0 1 0 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "mnist = load_digits(2)\n",
    "\n",
    "X,y = mnist.data, mnist.target\n",
    "\n",
    "\n",
    "print (\"y [shape - %s]:\"%(str(y.shape)),y[:10])\n",
    "\n",
    "print (\"X [shape - %s]:\"%(str(X.shape)))\n",
    "print (X[:3])\n",
    "print (y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# inputs and shareds\n",
    "shared_weights = theano.shared(np.zeros(X.shape[1], dtype='float64'))\n",
    "\n",
    "input_X = T.matrix('X', dtype='float64')\n",
    "input_y = T.vector('y', dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predicted_y = 1 / (1 + T.exp(-T.dot(input_X, shared_weights)))\n",
    "\n",
    "eps = 10**-9\n",
    "loss = -(input_y * T.log(predicted_y + eps) + (1 - input_y) * T.log(1 - predicted_y + eps)).mean()\n",
    "\n",
    "\n",
    "grad = T.grad(loss, shared_weights)\n",
    "\n",
    "alpha = 0.5 # learning rate\n",
    "\n",
    "updates = {\n",
    "    shared_weights: shared_weights - alpha * grad\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_function = theano.function(inputs=[input_X, input_y], outputs=loss, allow_input_downcast=True, updates=updates)\n",
    "\n",
    "predict_function = theano.function(inputs=[input_X], outputs=predicted_y, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.virtualenvs/mlyt/lib/python3.4/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 0:0.6931\n",
      "train auc: 1.0\n",
      "test auc: 1.0\n",
      "loss at iter 1:0.1544\n",
      "train auc: 1.0\n",
      "test auc: 1.0\n",
      "loss at iter 2:0.1451\n",
      "train auc: 1.0\n",
      "test auc: 1.0\n",
      "loss at iter 3:0.0892\n",
      "train auc: 1.0\n",
      "test auc: 1.0\n",
      "loss at iter 4:0.0029\n",
      "train auc: 1.0\n",
      "test auc: 1.0\n",
      "resulting weights:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f0c118a4da0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATAAAAD8CAYAAADwpviIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIoqVBBYRxUoCi4hiJYFFRLH+H3YElIAUZijiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0c11910d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "for i in range(5):\n",
    "    loss_i = train_function(X_train,y_train)\n",
    "    print (\"loss at iter %i:%.4f\"%(i,loss_i))\n",
    "    print (\"train auc:\",roc_auc_score(y_train,predict_function(X_train)))\n",
    "    print (\"test auc:\",roc_auc_score(y_test,predict_function(X_test)))\n",
    "\n",
    "    \n",
    "print (\"resulting weights:\")\n",
    "plt.imshow(shared_weights.get_value().reshape(8,-1))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# lasagne\n",
    "* lasagne is a library for neural network building and training\n",
    "* it's a low-level library with almost seamless integration with theano\n",
    "\n",
    "For a demo we shall solve the same digit recognition problem, but at a different scale\n",
    "* images are now 28x28\n",
    "* 10 different digits\n",
    "* 50k samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1, 28, 28) (50000,)\n"
     ]
    }
   ],
   "source": [
    "from mnist import load_dataset\n",
    "X_train,y_train,X_val,y_val,X_test,y_test = load_dataset()\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "\n",
    "input_X = T.tensor4(\"X\")\n",
    "\n",
    "#input dimention (None means \"Arbitrary\" and only works at  the first axes [samples])\n",
    "input_shape = [None,1,28,28]\n",
    "\n",
    "target_y = T.vector(\"target Y integer\",dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Defining network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Input layer (auxilary)\n",
    "input_layer = lasagne.layers.InputLayer(shape=input_shape, input_var=input_X)\n",
    "\n",
    "#fully connected layer, that takes input layer and applies 50 neurons to it.\n",
    "# nonlinearity here is sigmoid as in logistic regression\n",
    "# you can give a name to each layer (optional)\n",
    "dense_1 = lasagne.layers.DenseLayer(input_layer,num_units=50,\n",
    "                                   nonlinearity = lasagne.nonlinearities.sigmoid,\n",
    "                                   name = \"hidden_dense_layer\")\n",
    "\n",
    "#fully connected output layer that takes dense_1 as input and has 10 neurons (1 for each digit)\n",
    "#We use softmax nonlinearity to make probabilities add up to 1\n",
    "dense_output = lasagne.layers.DenseLayer(dense_1,num_units = 10,\n",
    "                                        nonlinearity = lasagne.nonlinearities.softmax,\n",
    "                                        name='output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#network prediction (theano-transformation)\n",
    "y_predicted = lasagne.layers.get_output(dense_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hidden_dense_layer.W, hidden_dense_layer.b, output.W, output.b]\n"
     ]
    }
   ],
   "source": [
    "#all network weights (shared variables)\n",
    "all_weights = lasagne.layers.get_all_params(dense_output)\n",
    "print (all_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Than you could simply\n",
    "* define loss function manually\n",
    "* compute error gradient over all weights\n",
    "* define updates\n",
    "* But that's a whole lot of work and life's short\n",
    "  * not to mention life's too short to wait for SGD to converge\n",
    "\n",
    "Instead, we shall use Lasagne builtins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Mean categorical crossentropy as a loss function - similar to logistic loss but for multiclass targets\n",
    "loss = lasagne.objectives.categorical_crossentropy(y_predicted,target_y).mean()\n",
    "\n",
    "#prediction accuracy\n",
    "accuracy = lasagne.objectives.categorical_accuracy(y_predicted,target_y).mean()\n",
    "\n",
    "#This function computes gradient AND composes weight updates just like you did earlier\n",
    "updates_sgd = lasagne.updates.sgd(loss, all_weights,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#function that computes loss and updates weights\n",
    "train_fun = theano.function([input_X,target_y],[loss,accuracy],updates= updates_sgd)\n",
    "\n",
    "#function that just computes accuracy\n",
    "accuracy_fun = theano.function([input_X,target_y],accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### That's all, now let's train it!\n",
    "* We got a lot of data, so it's recommended that you use SGD\n",
    "* So let's implement a function that splits the training sample into minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage.interpolation import rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# An auxilary function that returns mini-batches for neural network training\n",
    "\n",
    "#Parameters\n",
    "# X - a tensor of images with shape (many, 1, 28, 28), e.g. X_train\n",
    "# y - a vector of answers for corresponding images e.g. Y_train\n",
    "#batch_size - a single number - the intended size of each batches\n",
    "\n",
    "#What do need to implement\n",
    "# 1) Shuffle data\n",
    "# - Gotta shuffle X and y the same way not to break the correspondence between X_i and y_i\n",
    "# 3) Split data into minibatches of batch_size\n",
    "# - If data size is not a multiple of batch_size, make one last batch smaller.\n",
    "# 4) return a list (or an iterator) of pairs\n",
    "# - (подгруппа картинок, ответы из y на эту подгруппу)\n",
    "def iterate_minibatches(X, y, batchsize, seed=None, roll=False):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    if roll:\n",
    "        X = X.copy()\n",
    "        batch_count = y.shape[0] // batchsize\n",
    "        for index in range(batch_count):\n",
    "            begin = index * batchsize\n",
    "            end = begin + batchsize\n",
    "        shift = (np.random.randint(-3, 4), np.random.randint(-3, 4))\n",
    "        X[begin:end] = np.roll(X[begin:end], shift, axis=(2, 3))\n",
    "        angle = np.random.random() * 30 - 15\n",
    "        new_X = rotate(X[begin:end], angle=angle, axes=(2, 3))\n",
    "        new_cor = (new_X.shape[2] - 28) // 2\n",
    "        X[begin:end] = new_X[:, :, new_cor:new_cor+28, new_cor:new_cor+28]\n",
    "    indices = np.random.permutation(y.shape[0])\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "    batch_count = y.shape[0] // batchsize\n",
    "    for index in range(batch_count):\n",
    "        begin = index * batchsize\n",
    "        end = begin + batchsize\n",
    "        X_y = X[begin:end]\n",
    "        yield (X_y, y[begin:end])\n",
    "    if y.shape[0] % batchsize != 0:\n",
    "        begin = batch_count * batchsize\n",
    "        X_y = X[begin:]\n",
    "        yield (X_y, y[begin:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 0.840s\n",
      "  training loss (in-iteration):\t\t1.853022\n",
      "  train accuracy:\t\t59.85 %\n",
      "  validation accuracy:\t\t76.86 %\n",
      "Epoch 2 of 100 took 0.816s\n",
      "  training loss (in-iteration):\t\t1.183643\n",
      "  train accuracy:\t\t78.42 %\n",
      "  validation accuracy:\t\t84.09 %\n",
      "Epoch 3 of 100 took 0.819s\n",
      "  training loss (in-iteration):\t\t0.852617\n",
      "  train accuracy:\t\t83.26 %\n",
      "  validation accuracy:\t\t86.63 %\n",
      "Epoch 4 of 100 took 0.825s\n",
      "  training loss (in-iteration):\t\t0.688815\n",
      "  train accuracy:\t\t85.39 %\n",
      "  validation accuracy:\t\t87.97 %\n",
      "Epoch 5 of 100 took 0.874s\n",
      "  training loss (in-iteration):\t\t0.594013\n",
      "  train accuracy:\t\t86.60 %\n",
      "  validation accuracy:\t\t88.73 %\n",
      "Epoch 6 of 100 took 0.849s\n",
      "  training loss (in-iteration):\t\t0.532685\n",
      "  train accuracy:\t\t87.49 %\n",
      "  validation accuracy:\t\t89.52 %\n",
      "Epoch 7 of 100 took 0.859s\n",
      "  training loss (in-iteration):\t\t0.489722\n",
      "  train accuracy:\t\t88.06 %\n",
      "  validation accuracy:\t\t89.94 %\n",
      "Epoch 8 of 100 took 0.833s\n",
      "  training loss (in-iteration):\t\t0.458092\n",
      "  train accuracy:\t\t88.53 %\n",
      "  validation accuracy:\t\t90.07 %\n",
      "Epoch 9 of 100 took 0.833s\n",
      "  training loss (in-iteration):\t\t0.433768\n",
      "  train accuracy:\t\t88.91 %\n",
      "  validation accuracy:\t\t90.41 %\n",
      "Epoch 10 of 100 took 0.829s\n",
      "  training loss (in-iteration):\t\t0.414273\n",
      "  train accuracy:\t\t89.26 %\n",
      "  validation accuracy:\t\t90.67 %\n",
      "Epoch 11 of 100 took 0.842s\n",
      "  training loss (in-iteration):\t\t0.398257\n",
      "  train accuracy:\t\t89.53 %\n",
      "  validation accuracy:\t\t90.76 %\n",
      "Epoch 12 of 100 took 0.840s\n",
      "  training loss (in-iteration):\t\t0.384823\n",
      "  train accuracy:\t\t89.75 %\n",
      "  validation accuracy:\t\t91.09 %\n",
      "Epoch 13 of 100 took 0.832s\n",
      "  training loss (in-iteration):\t\t0.373313\n",
      "  train accuracy:\t\t90.00 %\n",
      "  validation accuracy:\t\t91.19 %\n",
      "Epoch 14 of 100 took 0.835s\n",
      "  training loss (in-iteration):\t\t0.363240\n",
      "  train accuracy:\t\t90.17 %\n",
      "  validation accuracy:\t\t91.29 %\n",
      "Epoch 15 of 100 took 0.833s\n",
      "  training loss (in-iteration):\t\t0.354340\n",
      "  train accuracy:\t\t90.36 %\n",
      "  validation accuracy:\t\t91.26 %\n",
      "Epoch 16 of 100 took 0.835s\n",
      "  training loss (in-iteration):\t\t0.346482\n",
      "  train accuracy:\t\t90.51 %\n",
      "  validation accuracy:\t\t91.49 %\n",
      "Epoch 17 of 100 took 0.838s\n",
      "  training loss (in-iteration):\t\t0.339259\n",
      "  train accuracy:\t\t90.61 %\n",
      "  validation accuracy:\t\t91.64 %\n",
      "Epoch 18 of 100 took 0.840s\n",
      "  training loss (in-iteration):\t\t0.332761\n",
      "  train accuracy:\t\t90.74 %\n",
      "  validation accuracy:\t\t91.74 %\n",
      "Epoch 19 of 100 took 0.855s\n",
      "  training loss (in-iteration):\t\t0.326840\n",
      "  train accuracy:\t\t90.89 %\n",
      "  validation accuracy:\t\t91.87 %\n",
      "Epoch 20 of 100 took 0.821s\n",
      "  training loss (in-iteration):\t\t0.321268\n",
      "  train accuracy:\t\t91.03 %\n",
      "  validation accuracy:\t\t91.95 %\n",
      "Epoch 21 of 100 took 0.829s\n",
      "  training loss (in-iteration):\t\t0.316044\n",
      "  train accuracy:\t\t91.13 %\n",
      "  validation accuracy:\t\t91.96 %\n",
      "Epoch 22 of 100 took 0.821s\n",
      "  training loss (in-iteration):\t\t0.311304\n",
      "  train accuracy:\t\t91.30 %\n",
      "  validation accuracy:\t\t92.15 %\n",
      "Epoch 23 of 100 took 0.822s\n",
      "  training loss (in-iteration):\t\t0.306850\n",
      "  train accuracy:\t\t91.38 %\n",
      "  validation accuracy:\t\t92.15 %\n",
      "Epoch 24 of 100 took 0.822s\n",
      "  training loss (in-iteration):\t\t0.302499\n",
      "  train accuracy:\t\t91.47 %\n",
      "  validation accuracy:\t\t92.19 %\n",
      "Epoch 25 of 100 took 0.820s\n",
      "  training loss (in-iteration):\t\t0.298502\n",
      "  train accuracy:\t\t91.56 %\n",
      "  validation accuracy:\t\t92.42 %\n",
      "Epoch 26 of 100 took 0.821s\n",
      "  training loss (in-iteration):\t\t0.294664\n",
      "  train accuracy:\t\t91.65 %\n",
      "  validation accuracy:\t\t92.43 %\n",
      "Epoch 27 of 100 took 0.828s\n",
      "  training loss (in-iteration):\t\t0.291041\n",
      "  train accuracy:\t\t91.74 %\n",
      "  validation accuracy:\t\t92.45 %\n",
      "Epoch 28 of 100 took 0.829s\n",
      "  training loss (in-iteration):\t\t0.287500\n",
      "  train accuracy:\t\t91.83 %\n",
      "  validation accuracy:\t\t92.58 %\n",
      "Epoch 29 of 100 took 0.785s\n",
      "  training loss (in-iteration):\t\t0.284233\n",
      "  train accuracy:\t\t91.91 %\n",
      "  validation accuracy:\t\t92.62 %\n",
      "Epoch 30 of 100 took 0.808s\n",
      "  training loss (in-iteration):\t\t0.281033\n",
      "  train accuracy:\t\t92.01 %\n",
      "  validation accuracy:\t\t92.74 %\n",
      "Epoch 31 of 100 took 0.819s\n",
      "  training loss (in-iteration):\t\t0.277853\n",
      "  train accuracy:\t\t92.10 %\n",
      "  validation accuracy:\t\t92.75 %\n",
      "Epoch 32 of 100 took 0.821s\n",
      "  training loss (in-iteration):\t\t0.274975\n",
      "  train accuracy:\t\t92.20 %\n",
      "  validation accuracy:\t\t92.78 %\n",
      "Epoch 33 of 100 took 0.816s\n",
      "  training loss (in-iteration):\t\t0.272050\n",
      "  train accuracy:\t\t92.29 %\n",
      "  validation accuracy:\t\t92.82 %\n",
      "Epoch 34 of 100 took 0.809s\n",
      "  training loss (in-iteration):\t\t0.269336\n",
      "  train accuracy:\t\t92.36 %\n",
      "  validation accuracy:\t\t92.89 %\n",
      "Epoch 35 of 100 took 0.829s\n",
      "  training loss (in-iteration):\t\t0.266695\n",
      "  train accuracy:\t\t92.44 %\n",
      "  validation accuracy:\t\t92.94 %\n",
      "Epoch 36 of 100 took 0.824s\n",
      "  training loss (in-iteration):\t\t0.264139\n",
      "  train accuracy:\t\t92.48 %\n",
      "  validation accuracy:\t\t93.07 %\n",
      "Epoch 37 of 100 took 0.809s\n",
      "  training loss (in-iteration):\t\t0.261600\n",
      "  train accuracy:\t\t92.57 %\n",
      "  validation accuracy:\t\t93.24 %\n",
      "Epoch 38 of 100 took 0.817s\n",
      "  training loss (in-iteration):\t\t0.259138\n",
      "  train accuracy:\t\t92.66 %\n",
      "  validation accuracy:\t\t93.20 %\n",
      "Epoch 39 of 100 took 0.772s\n",
      "  training loss (in-iteration):\t\t0.256709\n",
      "  train accuracy:\t\t92.74 %\n",
      "  validation accuracy:\t\t93.26 %\n",
      "Epoch 40 of 100 took 0.780s\n",
      "  training loss (in-iteration):\t\t0.254478\n",
      "  train accuracy:\t\t92.79 %\n",
      "  validation accuracy:\t\t93.31 %\n",
      "Epoch 41 of 100 took 0.783s\n",
      "  training loss (in-iteration):\t\t0.252205\n",
      "  train accuracy:\t\t92.85 %\n",
      "  validation accuracy:\t\t93.39 %\n",
      "Epoch 42 of 100 took 0.780s\n",
      "  training loss (in-iteration):\t\t0.250007\n",
      "  train accuracy:\t\t92.90 %\n",
      "  validation accuracy:\t\t93.39 %\n",
      "Epoch 43 of 100 took 0.791s\n",
      "  training loss (in-iteration):\t\t0.247920\n",
      "  train accuracy:\t\t92.98 %\n",
      "  validation accuracy:\t\t93.46 %\n",
      "Epoch 44 of 100 took 0.785s\n",
      "  training loss (in-iteration):\t\t0.245830\n",
      "  train accuracy:\t\t93.02 %\n",
      "  validation accuracy:\t\t93.48 %\n",
      "Epoch 45 of 100 took 0.792s\n",
      "  training loss (in-iteration):\t\t0.243760\n",
      "  train accuracy:\t\t93.07 %\n",
      "  validation accuracy:\t\t93.57 %\n",
      "Epoch 46 of 100 took 0.804s\n",
      "  training loss (in-iteration):\t\t0.241812\n",
      "  train accuracy:\t\t93.10 %\n",
      "  validation accuracy:\t\t93.57 %\n",
      "Epoch 47 of 100 took 0.789s\n",
      "  training loss (in-iteration):\t\t0.239835\n",
      "  train accuracy:\t\t93.19 %\n",
      "  validation accuracy:\t\t93.63 %\n",
      "Epoch 48 of 100 took 0.833s\n",
      "  training loss (in-iteration):\t\t0.237913\n",
      "  train accuracy:\t\t93.24 %\n",
      "  validation accuracy:\t\t93.70 %\n",
      "Epoch 49 of 100 took 0.793s\n",
      "  training loss (in-iteration):\t\t0.236114\n",
      "  train accuracy:\t\t93.31 %\n",
      "  validation accuracy:\t\t93.73 %\n",
      "Epoch 50 of 100 took 0.786s\n",
      "  training loss (in-iteration):\t\t0.234237\n",
      "  train accuracy:\t\t93.34 %\n",
      "  validation accuracy:\t\t93.73 %\n",
      "Epoch 51 of 100 took 0.798s\n",
      "  training loss (in-iteration):\t\t0.232494\n",
      "  train accuracy:\t\t93.39 %\n",
      "  validation accuracy:\t\t93.85 %\n",
      "Epoch 52 of 100 took 0.785s\n",
      "  training loss (in-iteration):\t\t0.230692\n",
      "  train accuracy:\t\t93.44 %\n",
      "  validation accuracy:\t\t93.90 %\n",
      "Epoch 53 of 100 took 0.780s\n",
      "  training loss (in-iteration):\t\t0.229007\n",
      "  train accuracy:\t\t93.49 %\n",
      "  validation accuracy:\t\t93.79 %\n",
      "Epoch 54 of 100 took 0.803s\n",
      "  training loss (in-iteration):\t\t0.227339\n",
      "  train accuracy:\t\t93.58 %\n",
      "  validation accuracy:\t\t93.96 %\n",
      "Epoch 55 of 100 took 0.924s\n",
      "  training loss (in-iteration):\t\t0.225701\n",
      "  train accuracy:\t\t93.59 %\n",
      "  validation accuracy:\t\t93.90 %\n",
      "Epoch 56 of 100 took 0.804s\n",
      "  training loss (in-iteration):\t\t0.224032\n",
      "  train accuracy:\t\t93.65 %\n",
      "  validation accuracy:\t\t94.00 %\n",
      "Epoch 57 of 100 took 0.826s\n",
      "  training loss (in-iteration):\t\t0.222462\n",
      "  train accuracy:\t\t93.68 %\n",
      "  validation accuracy:\t\t93.98 %\n",
      "Epoch 58 of 100 took 0.788s\n",
      "  training loss (in-iteration):\t\t0.220919\n",
      "  train accuracy:\t\t93.74 %\n",
      "  validation accuracy:\t\t94.06 %\n",
      "Epoch 59 of 100 took 0.793s\n",
      "  training loss (in-iteration):\t\t0.219355\n",
      "  train accuracy:\t\t93.75 %\n",
      "  validation accuracy:\t\t94.15 %\n",
      "Epoch 60 of 100 took 0.810s\n",
      "  training loss (in-iteration):\t\t0.217886\n",
      "  train accuracy:\t\t93.83 %\n",
      "  validation accuracy:\t\t94.18 %\n",
      "Epoch 61 of 100 took 0.780s\n",
      "  training loss (in-iteration):\t\t0.216360\n",
      "  train accuracy:\t\t93.87 %\n",
      "  validation accuracy:\t\t94.20 %\n",
      "Epoch 62 of 100 took 0.795s\n",
      "  training loss (in-iteration):\t\t0.214888\n",
      "  train accuracy:\t\t93.92 %\n",
      "  validation accuracy:\t\t94.23 %\n",
      "Epoch 63 of 100 took 0.789s\n",
      "  training loss (in-iteration):\t\t0.213435\n",
      "  train accuracy:\t\t93.97 %\n",
      "  validation accuracy:\t\t94.23 %\n",
      "Epoch 64 of 100 took 0.815s\n",
      "  training loss (in-iteration):\t\t0.211995\n",
      "  train accuracy:\t\t94.00 %\n",
      "  validation accuracy:\t\t94.28 %\n",
      "Epoch 65 of 100 took 0.783s\n",
      "  training loss (in-iteration):\t\t0.210620\n",
      "  train accuracy:\t\t94.04 %\n",
      "  validation accuracy:\t\t94.32 %\n",
      "Epoch 66 of 100 took 0.811s\n",
      "  training loss (in-iteration):\t\t0.209166\n",
      "  train accuracy:\t\t94.06 %\n",
      "  validation accuracy:\t\t94.34 %\n",
      "Epoch 67 of 100 took 0.787s\n",
      "  training loss (in-iteration):\t\t0.207925\n",
      "  train accuracy:\t\t94.09 %\n",
      "  validation accuracy:\t\t94.43 %\n",
      "Epoch 68 of 100 took 0.782s\n",
      "  training loss (in-iteration):\t\t0.206521\n",
      "  train accuracy:\t\t94.15 %\n",
      "  validation accuracy:\t\t94.40 %\n",
      "Epoch 69 of 100 took 0.779s\n",
      "  training loss (in-iteration):\t\t0.205243\n",
      "  train accuracy:\t\t94.19 %\n",
      "  validation accuracy:\t\t94.47 %\n",
      "Epoch 70 of 100 took 0.816s\n",
      "  training loss (in-iteration):\t\t0.203922\n",
      "  train accuracy:\t\t94.20 %\n",
      "  validation accuracy:\t\t94.43 %\n",
      "Epoch 71 of 100 took 0.793s\n",
      "  training loss (in-iteration):\t\t0.202687\n",
      "  train accuracy:\t\t94.22 %\n",
      "  validation accuracy:\t\t94.52 %\n",
      "Epoch 72 of 100 took 0.945s\n",
      "  training loss (in-iteration):\t\t0.201403\n",
      "  train accuracy:\t\t94.26 %\n",
      "  validation accuracy:\t\t94.60 %\n",
      "Epoch 73 of 100 took 0.807s\n",
      "  training loss (in-iteration):\t\t0.200193\n",
      "  train accuracy:\t\t94.31 %\n",
      "  validation accuracy:\t\t94.58 %\n",
      "Epoch 74 of 100 took 0.798s\n",
      "  training loss (in-iteration):\t\t0.198942\n",
      "  train accuracy:\t\t94.35 %\n",
      "  validation accuracy:\t\t94.70 %\n",
      "Epoch 75 of 100 took 0.833s\n",
      "  training loss (in-iteration):\t\t0.197776\n",
      "  train accuracy:\t\t94.39 %\n",
      "  validation accuracy:\t\t94.73 %\n",
      "Epoch 76 of 100 took 0.791s\n",
      "  training loss (in-iteration):\t\t0.196569\n",
      "  train accuracy:\t\t94.41 %\n",
      "  validation accuracy:\t\t94.70 %\n",
      "Epoch 77 of 100 took 0.799s\n",
      "  training loss (in-iteration):\t\t0.195409\n",
      "  train accuracy:\t\t94.43 %\n",
      "  validation accuracy:\t\t94.73 %\n",
      "Epoch 78 of 100 took 0.807s\n",
      "  training loss (in-iteration):\t\t0.194223\n",
      "  train accuracy:\t\t94.45 %\n",
      "  validation accuracy:\t\t94.78 %\n",
      "Epoch 79 of 100 took 0.792s\n",
      "  training loss (in-iteration):\t\t0.193067\n",
      "  train accuracy:\t\t94.50 %\n",
      "  validation accuracy:\t\t94.76 %\n",
      "Epoch 80 of 100 took 0.798s\n",
      "  training loss (in-iteration):\t\t0.191913\n",
      "  train accuracy:\t\t94.54 %\n",
      "  validation accuracy:\t\t94.78 %\n",
      "Epoch 81 of 100 took 0.804s\n",
      "  training loss (in-iteration):\t\t0.190800\n",
      "  train accuracy:\t\t94.56 %\n",
      "  validation accuracy:\t\t94.86 %\n",
      "Epoch 82 of 100 took 0.793s\n",
      "  training loss (in-iteration):\t\t0.189738\n",
      "  train accuracy:\t\t94.61 %\n",
      "  validation accuracy:\t\t94.84 %\n",
      "Epoch 83 of 100 took 0.805s\n",
      "  training loss (in-iteration):\t\t0.188621\n",
      "  train accuracy:\t\t94.65 %\n",
      "  validation accuracy:\t\t94.88 %\n",
      "Epoch 84 of 100 took 0.801s\n",
      "  training loss (in-iteration):\t\t0.187601\n",
      "  train accuracy:\t\t94.66 %\n",
      "  validation accuracy:\t\t94.89 %\n",
      "Epoch 85 of 100 took 0.801s\n",
      "  training loss (in-iteration):\t\t0.186543\n",
      "  train accuracy:\t\t94.69 %\n",
      "  validation accuracy:\t\t94.93 %\n",
      "Epoch 86 of 100 took 0.799s\n",
      "  training loss (in-iteration):\t\t0.185473\n",
      "  train accuracy:\t\t94.72 %\n",
      "  validation accuracy:\t\t94.94 %\n",
      "Epoch 87 of 100 took 0.791s\n",
      "  training loss (in-iteration):\t\t0.184427\n",
      "  train accuracy:\t\t94.76 %\n",
      "  validation accuracy:\t\t95.02 %\n",
      "Epoch 88 of 100 took 0.802s\n",
      "  training loss (in-iteration):\t\t0.183459\n",
      "  train accuracy:\t\t94.79 %\n",
      "  validation accuracy:\t\t95.03 %\n",
      "Epoch 89 of 100 took 0.795s\n",
      "  training loss (in-iteration):\t\t0.182423\n",
      "  train accuracy:\t\t94.81 %\n",
      "  validation accuracy:\t\t95.03 %\n",
      "Epoch 90 of 100 took 0.792s\n",
      "  training loss (in-iteration):\t\t0.181435\n",
      "  train accuracy:\t\t94.85 %\n",
      "  validation accuracy:\t\t95.06 %\n",
      "Epoch 91 of 100 took 0.805s\n",
      "  training loss (in-iteration):\t\t0.180471\n",
      "  train accuracy:\t\t94.89 %\n",
      "  validation accuracy:\t\t95.14 %\n",
      "Epoch 92 of 100 took 0.796s\n",
      "  training loss (in-iteration):\t\t0.179483\n",
      "  train accuracy:\t\t94.90 %\n",
      "  validation accuracy:\t\t95.08 %\n",
      "Epoch 93 of 100 took 0.840s\n",
      "  training loss (in-iteration):\t\t0.178552\n",
      "  train accuracy:\t\t94.91 %\n",
      "  validation accuracy:\t\t95.11 %\n",
      "Epoch 94 of 100 took 0.796s\n",
      "  training loss (in-iteration):\t\t0.177555\n",
      "  train accuracy:\t\t94.95 %\n",
      "  validation accuracy:\t\t95.14 %\n",
      "Epoch 95 of 100 took 0.792s\n",
      "  training loss (in-iteration):\t\t0.176650\n",
      "  train accuracy:\t\t94.96 %\n",
      "  validation accuracy:\t\t95.20 %\n",
      "Epoch 96 of 100 took 0.799s\n",
      "  training loss (in-iteration):\t\t0.175718\n",
      "  train accuracy:\t\t95.00 %\n",
      "  validation accuracy:\t\t95.13 %\n",
      "Epoch 97 of 100 took 0.795s\n",
      "  training loss (in-iteration):\t\t0.174802\n",
      "  train accuracy:\t\t95.05 %\n",
      "  validation accuracy:\t\t95.22 %\n",
      "Epoch 98 of 100 took 0.791s\n",
      "  training loss (in-iteration):\t\t0.173915\n",
      "  train accuracy:\t\t95.06 %\n",
      "  validation accuracy:\t\t95.22 %\n",
      "Epoch 99 of 100 took 0.793s\n",
      "  training loss (in-iteration):\t\t0.172988\n",
      "  train accuracy:\t\t95.11 %\n",
      "  validation accuracy:\t\t95.25 %\n",
      "Epoch 100 of 100 took 0.800s\n",
      "  training loss (in-iteration):\t\t0.172122\n",
      "  train accuracy:\t\t95.12 %\n",
      "  validation accuracy:\t\t95.25 %\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 100 #amount of passes through the data\n",
    "\n",
    "batch_size = 50 #number of samples processed at each function call\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train,batch_size):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch= train_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets)\n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test accuracy:\t\t98.08 %\n",
      "We need more magic!\n"
     ]
    }
   ],
   "source": [
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    inputs, targets = batch\n",
    "    acc = accuracy_fun(inputs, targets)\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))\n",
    "\n",
    "if test_acc / test_batches * 100 > 99:\n",
    "    print (\"Achievement unlocked: 80lvl Warlock!\")\n",
    "else:\n",
    "    print (\"We need more magic!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# A better network ( 4+ pts )\n",
    "\n",
    "\n",
    "* The quest is to create a network that gets at least 99% at test set\n",
    " * In case you tried several architectures and have a __detailed__ report - 97.5% \"is fine too\". \n",
    " * __+1 bonus point__ each 0.2% past 99%\n",
    " * More points for creative approach\n",
    " \n",
    "__ There is a mini-report at the end that you will have to fill in. We recommend to read it first and fill in while you are iterating. __\n",
    " \n",
    "\n",
    "## Tips on what can be done:\n",
    "\n",
    "\n",
    "\n",
    " * Network size\n",
    "   * MOAR neurons, \n",
    "   * MOAR layers, \n",
    "   * Convolutions are almost imperative\n",
    "   * Пх'нглуи мглв'нафх Ктулху Р'льех вгах'нагл фхтагн! \n",
    "   \n",
    "   \n",
    "   \n",
    " * Regularize to prevent overfitting\n",
    "   * Add some L2 weight norm to the loss function, theano will do the rest\n",
    "   * Can be done manually or via - http://lasagne.readthedocs.org/en/latest/modules/regularization.html\n",
    "   \n",
    "   \n",
    "   \n",
    " * Better optimization - rmsprop, nesterov_momentum, adadelta, adagrad and so on.\n",
    "   * Converge faster and sometimes reach better optima\n",
    "   * It might make sense to tweak learning rate, other learning parameters, batch size and number of epochs\n",
    "   \n",
    "   \n",
    "   \n",
    " * Dropout - to prevent overfitting\n",
    "   * `lasagne.layers.DropoutLayer(prev_layer, p=probability_to_zero_out)`\n",
    "   \n",
    "   \n",
    "   \n",
    " * Convolution layers\n",
    "   * `network = lasagne.layers.Conv2DLayer(prev_layer,`\n",
    "    `                       num_filters = n_neurons,`\n",
    "    `                        filter_size = (filter width, filter height),`\n",
    "    `                        nonlinearity = some_nonlinearity)`\n",
    "   * Warning! Training convolutional networks can take long without GPU.\n",
    "     * If you are CPU-only, we still recomment to try a simple convolutional architecture\n",
    "     * a perfect option is if you can set it up to run at nighttime and check it up at the morning.\n",
    " \n",
    " * Plenty other layers and architectures\n",
    "   * http://lasagne.readthedocs.org/en/latest/modules/layers.html\n",
    "   * batch normalization, pooling, etc\n",
    "   \n",
    "   \n",
    " * Nonlinearities in the hidden layers\n",
    "   * tanh, relu, leaky relu, etc\n",
    "   \n",
    " \n",
    " \n",
    "   \n",
    "There is a template for your solution below that you can opt to use or throw away and write it your way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1, 28, 28) (50000,)\n"
     ]
    }
   ],
   "source": [
    "from mnist import load_dataset\n",
    "X_train,y_train,X_val,y_val,X_test,y_test = load_dataset()\n",
    "\n",
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0be8001978>"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH0iK8ANJEX4gqf8HKCrbKPvW0lcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0bdfb414e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(rotate(X_train[110, 0], angle=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "\n",
    "input_X = T.tensor4(\"X\")\n",
    "\n",
    "#input dimention (None means \"Arbitrary\" and only works at  the first axes [samples])\n",
    "input_shape = [None,1,28,28]\n",
    "\n",
    "target_y = T.vector(\"target Y integer\",dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Input layer (auxilary)\n",
    "input_layer = lasagne.layers.InputLayer(shape = input_shape,input_var=input_X)\n",
    "\n",
    "# conv block\n",
    "conv3 = lasagne.layers.Conv2DLayer(\n",
    "    input_layer, num_filters=8,\n",
    "    filter_size=(3, 3), nonlinearity=lasagne.nonlinearities.rectify,\n",
    "    pad='same',\n",
    ")\n",
    "conv5 = lasagne.layers.Conv2DLayer(\n",
    "    input_layer, num_filters=4,\n",
    "    filter_size=(5, 5), nonlinearity=lasagne.nonlinearities.rectify,\n",
    "    pad='same',\n",
    ")\n",
    "\n",
    "network = lasagne.layers.ConcatLayer([conv3, conv5])\n",
    "network = lasagne.layers.Pool2DLayer(network, pool_size=(2, 2), ignore_border=False)\n",
    "network = lasagne.layers.BatchNormLayer(network)\n",
    "\n",
    "# conv block\n",
    "network = lasagne.layers.Conv2DLayer(\n",
    "    network, num_filters=16,\n",
    "    filter_size=(3, 3), nonlinearity=lasagne.nonlinearities.rectify,\n",
    "    #pad='same',\n",
    ")\n",
    "network = lasagne.layers.Pool2DLayer(network, pool_size=(2, 2), ignore_border=True)\n",
    "network = lasagne.layers.BatchNormLayer(network)\n",
    "\n",
    "# output\n",
    "network = lasagne.layers.DenseLayer(\n",
    "    network, num_units=10,\n",
    "    nonlinearity=lasagne.nonlinearities.softmax,\n",
    "    name='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Network predictions (theano-transformation)\n",
    "y_predicted = lasagne.layers.get_output(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W, b, W, b, beta, gamma, W, b, beta, gamma, output.W, output.b]\n"
     ]
    }
   ],
   "source": [
    "#All weights (shared-varaibles)\n",
    "# \"trainable\" flag means not to return auxilary params like batch mean (for batch normalization)\n",
    "all_weights = lasagne.layers.get_all_params(network,trainable=True)\n",
    "print (all_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss = lasagne.objectives.categorical_crossentropy(y_predicted,target_y).mean()\\\n",
    "       + T.sum([(x ** 2).sum() for x in all_weights]) * 10 ** -5\n",
    "\n",
    "#prediction accuracy\n",
    "accuracy = lasagne.objectives.categorical_accuracy(y_predicted,target_y).mean()\n",
    "\n",
    "updates = lasagne.updates.adam(loss, all_weights,learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#A function that accepts X and y, returns loss functions and performs weight updates\n",
    "train_fun = theano.function([input_X,target_y],[loss,accuracy],updates=updates)\n",
    "\n",
    "#A function that just computes accuracy given X and y\n",
    "accuracy_fun = theano.function([input_X,target_y],accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_model(num_epochs, batch_size):\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_acc = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for index, batch in enumerate(iterate_minibatches(X_train, y_train,batch_size, roll=True)):\n",
    "            # print('batch_num=', index)\n",
    "            inputs, targets = batch\n",
    "            train_err_batch, train_acc_batch= train_fun(inputs, targets)\n",
    "            train_err += train_err_batch\n",
    "            train_acc += train_acc_batch\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "            inputs, targets = batch\n",
    "            val_acc += accuracy_fun(inputs, targets)\n",
    "            val_batches += 1\n",
    "\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "        print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "            train_acc / train_batches * 100))\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            val_acc / val_batches * 100))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 26.323s\n",
      "  training loss (in-iteration):\t\t0.662212\n",
      "  train accuracy:\t\t80.21 %\n",
      "  validation accuracy:\t\t95.05 %\n",
      "Epoch 2 of 100 took 26.151s\n",
      "  training loss (in-iteration):\t\t0.177241\n",
      "  train accuracy:\t\t95.04 %\n",
      "  validation accuracy:\t\t97.25 %\n",
      "Epoch 3 of 100 took 25.647s\n",
      "  training loss (in-iteration):\t\t0.117433\n",
      "  train accuracy:\t\t96.61 %\n",
      "  validation accuracy:\t\t97.66 %\n",
      "Epoch 4 of 100 took 24.571s\n",
      "  training loss (in-iteration):\t\t0.081947\n",
      "  train accuracy:\t\t97.65 %\n",
      "  validation accuracy:\t\t98.04 %\n",
      "Epoch 5 of 100 took 25.612s\n",
      "  training loss (in-iteration):\t\t0.077182\n",
      "  train accuracy:\t\t97.71 %\n",
      "  validation accuracy:\t\t98.14 %\n",
      "Epoch 6 of 100 took 25.393s\n",
      "  training loss (in-iteration):\t\t0.062749\n",
      "  train accuracy:\t\t98.16 %\n",
      "  validation accuracy:\t\t98.33 %\n",
      "Epoch 7 of 100 took 25.284s\n",
      "  training loss (in-iteration):\t\t0.053088\n",
      "  train accuracy:\t\t98.51 %\n",
      "  validation accuracy:\t\t98.42 %\n",
      "Epoch 8 of 100 took 25.823s\n",
      "  training loss (in-iteration):\t\t0.048547\n",
      "  train accuracy:\t\t98.61 %\n",
      "  validation accuracy:\t\t98.51 %\n",
      "Epoch 9 of 100 took 26.276s\n",
      "  training loss (in-iteration):\t\t0.041029\n",
      "  train accuracy:\t\t98.84 %\n",
      "  validation accuracy:\t\t98.54 %\n",
      "Epoch 10 of 100 took 25.733s\n",
      "  training loss (in-iteration):\t\t0.037573\n",
      "  train accuracy:\t\t98.97 %\n",
      "  validation accuracy:\t\t98.52 %\n",
      "Epoch 11 of 100 took 26.214s\n",
      "  training loss (in-iteration):\t\t0.035461\n",
      "  train accuracy:\t\t98.98 %\n",
      "  validation accuracy:\t\t98.58 %\n",
      "Epoch 12 of 100 took 25.673s\n",
      "  training loss (in-iteration):\t\t0.033436\n",
      "  train accuracy:\t\t99.05 %\n",
      "  validation accuracy:\t\t98.56 %\n",
      "Epoch 13 of 100 took 25.164s\n",
      "  training loss (in-iteration):\t\t0.033724\n",
      "  train accuracy:\t\t99.03 %\n",
      "  validation accuracy:\t\t98.61 %\n",
      "Epoch 14 of 100 took 27.268s\n",
      "  training loss (in-iteration):\t\t0.036868\n",
      "  train accuracy:\t\t98.96 %\n",
      "  validation accuracy:\t\t98.60 %\n",
      "Epoch 15 of 100 took 25.828s\n",
      "  training loss (in-iteration):\t\t0.031315\n",
      "  train accuracy:\t\t99.12 %\n",
      "  validation accuracy:\t\t98.67 %\n",
      "Epoch 16 of 100 took 25.733s\n",
      "  training loss (in-iteration):\t\t0.026051\n",
      "  train accuracy:\t\t99.30 %\n",
      "  validation accuracy:\t\t98.64 %\n",
      "Epoch 17 of 100 took 25.477s\n",
      "  training loss (in-iteration):\t\t0.022897\n",
      "  train accuracy:\t\t99.43 %\n",
      "  validation accuracy:\t\t98.70 %\n",
      "Epoch 18 of 100 took 25.896s\n",
      "  training loss (in-iteration):\t\t0.021124\n",
      "  train accuracy:\t\t99.47 %\n",
      "  validation accuracy:\t\t98.65 %\n",
      "Epoch 19 of 100 took 25.094s\n",
      "  training loss (in-iteration):\t\t0.019532\n",
      "  train accuracy:\t\t99.51 %\n",
      "  validation accuracy:\t\t98.71 %\n",
      "Epoch 20 of 100 took 25.556s\n",
      "  training loss (in-iteration):\t\t0.019932\n",
      "  train accuracy:\t\t99.52 %\n",
      "  validation accuracy:\t\t98.71 %\n",
      "Epoch 21 of 100 took 25.600s\n",
      "  training loss (in-iteration):\t\t0.017787\n",
      "  train accuracy:\t\t99.57 %\n",
      "  validation accuracy:\t\t98.72 %\n",
      "Epoch 22 of 100 took 25.160s\n",
      "  training loss (in-iteration):\t\t0.017550\n",
      "  train accuracy:\t\t99.58 %\n",
      "  validation accuracy:\t\t98.69 %\n",
      "Epoch 23 of 100 took 26.188s\n",
      "  training loss (in-iteration):\t\t0.015317\n",
      "  train accuracy:\t\t99.64 %\n",
      "  validation accuracy:\t\t98.71 %\n",
      "Epoch 24 of 100 took 25.656s\n",
      "  training loss (in-iteration):\t\t0.015338\n",
      "  train accuracy:\t\t99.66 %\n",
      "  validation accuracy:\t\t98.74 %\n",
      "Epoch 25 of 100 took 25.482s\n",
      "  training loss (in-iteration):\t\t0.021521\n",
      "  train accuracy:\t\t99.48 %\n",
      "  validation accuracy:\t\t98.81 %\n",
      "Epoch 26 of 100 took 25.579s\n",
      "  training loss (in-iteration):\t\t0.020788\n",
      "  train accuracy:\t\t99.47 %\n",
      "  validation accuracy:\t\t98.73 %\n",
      "Epoch 27 of 100 took 25.306s\n",
      "  training loss (in-iteration):\t\t0.013378\n",
      "  train accuracy:\t\t99.73 %\n",
      "  validation accuracy:\t\t98.75 %\n",
      "Epoch 28 of 100 took 24.650s\n",
      "  training loss (in-iteration):\t\t0.013921\n",
      "  train accuracy:\t\t99.70 %\n",
      "  validation accuracy:\t\t98.77 %\n",
      "Epoch 29 of 100 took 25.007s\n",
      "  training loss (in-iteration):\t\t0.030162\n",
      "  train accuracy:\t\t99.25 %\n",
      "  validation accuracy:\t\t98.77 %\n",
      "Epoch 30 of 100 took 25.080s\n",
      "  training loss (in-iteration):\t\t0.019406\n",
      "  train accuracy:\t\t99.51 %\n",
      "  validation accuracy:\t\t98.79 %\n",
      "Epoch 31 of 100 took 24.022s\n",
      "  training loss (in-iteration):\t\t0.012779\n",
      "  train accuracy:\t\t99.76 %\n",
      "  validation accuracy:\t\t98.85 %\n",
      "Epoch 32 of 100 took 23.993s\n",
      "  training loss (in-iteration):\t\t0.009196\n",
      "  train accuracy:\t\t99.85 %\n",
      "  validation accuracy:\t\t98.78 %\n",
      "Epoch 33 of 100 took 25.497s\n",
      "  training loss (in-iteration):\t\t0.016221\n",
      "  train accuracy:\t\t99.65 %\n",
      "  validation accuracy:\t\t98.79 %\n",
      "Epoch 34 of 100 took 25.830s\n",
      "  training loss (in-iteration):\t\t0.009347\n",
      "  train accuracy:\t\t99.83 %\n",
      "  validation accuracy:\t\t98.86 %\n",
      "Epoch 35 of 100 took 24.902s\n",
      "  training loss (in-iteration):\t\t0.008198\n",
      "  train accuracy:\t\t99.90 %\n",
      "  validation accuracy:\t\t98.79 %\n",
      "Epoch 36 of 100 took 26.061s\n",
      "  training loss (in-iteration):\t\t0.009482\n",
      "  train accuracy:\t\t99.86 %\n",
      "  validation accuracy:\t\t98.83 %\n",
      "Epoch 37 of 100 took 25.512s\n",
      "  training loss (in-iteration):\t\t0.009587\n",
      "  train accuracy:\t\t99.85 %\n",
      "  validation accuracy:\t\t98.80 %\n",
      "Epoch 38 of 100 took 24.700s\n",
      "  training loss (in-iteration):\t\t0.011705\n",
      "  train accuracy:\t\t99.76 %\n",
      "  validation accuracy:\t\t98.84 %\n",
      "Epoch 39 of 100 took 25.547s\n",
      "  training loss (in-iteration):\t\t0.007157\n",
      "  train accuracy:\t\t99.90 %\n",
      "  validation accuracy:\t\t98.82 %\n",
      "Epoch 40 of 100 took 25.287s\n",
      "  training loss (in-iteration):\t\t0.008406\n",
      "  train accuracy:\t\t99.89 %\n",
      "  validation accuracy:\t\t98.68 %\n",
      "Epoch 41 of 100 took 25.818s\n",
      "  training loss (in-iteration):\t\t0.013801\n",
      "  train accuracy:\t\t99.72 %\n",
      "  validation accuracy:\t\t98.84 %\n",
      "Epoch 42 of 100 took 25.839s\n",
      "  training loss (in-iteration):\t\t0.011685\n",
      "  train accuracy:\t\t99.75 %\n",
      "  validation accuracy:\t\t98.75 %\n",
      "Epoch 43 of 100 took 25.793s\n",
      "  training loss (in-iteration):\t\t0.016223\n",
      "  train accuracy:\t\t99.67 %\n",
      "  validation accuracy:\t\t98.79 %\n",
      "Epoch 44 of 100 took 26.073s\n",
      "  training loss (in-iteration):\t\t0.007096\n",
      "  train accuracy:\t\t99.90 %\n",
      "  validation accuracy:\t\t98.83 %\n",
      "Epoch 45 of 100 took 26.402s\n",
      "  training loss (in-iteration):\t\t0.006770\n",
      "  train accuracy:\t\t99.93 %\n",
      "  validation accuracy:\t\t98.82 %\n",
      "Epoch 46 of 100 took 26.316s\n",
      "  training loss (in-iteration):\t\t0.008553\n",
      "  train accuracy:\t\t99.87 %\n",
      "  validation accuracy:\t\t98.78 %\n",
      "Epoch 47 of 100 took 26.263s\n",
      "  training loss (in-iteration):\t\t0.005017\n",
      "  train accuracy:\t\t99.96 %\n",
      "  validation accuracy:\t\t98.87 %\n",
      "Epoch 48 of 100 took 26.105s\n",
      "  training loss (in-iteration):\t\t0.006439\n",
      "  train accuracy:\t\t99.92 %\n",
      "  validation accuracy:\t\t98.84 %\n",
      "Epoch 49 of 100 took 25.981s\n",
      "  training loss (in-iteration):\t\t0.006969\n",
      "  train accuracy:\t\t99.91 %\n",
      "  validation accuracy:\t\t98.79 %\n",
      "Epoch 50 of 100 took 24.746s\n",
      "  training loss (in-iteration):\t\t0.013525\n",
      "  train accuracy:\t\t99.72 %\n",
      "  validation accuracy:\t\t98.76 %\n",
      "Epoch 51 of 100 took 25.236s\n",
      "  training loss (in-iteration):\t\t0.007820\n",
      "  train accuracy:\t\t99.87 %\n",
      "  validation accuracy:\t\t98.86 %\n",
      "Epoch 52 of 100 took 25.712s\n",
      "  training loss (in-iteration):\t\t0.009434\n",
      "  train accuracy:\t\t99.85 %\n",
      "  validation accuracy:\t\t98.78 %\n",
      "Epoch 53 of 100 took 25.941s\n",
      "  training loss (in-iteration):\t\t0.007213\n",
      "  train accuracy:\t\t99.90 %\n",
      "  validation accuracy:\t\t98.87 %\n",
      "Epoch 54 of 100 took 26.034s\n",
      "  training loss (in-iteration):\t\t0.007165\n",
      "  train accuracy:\t\t99.88 %\n",
      "  validation accuracy:\t\t98.84 %\n",
      "Epoch 55 of 100 took 25.956s\n",
      "  training loss (in-iteration):\t\t0.009515\n",
      "  train accuracy:\t\t99.80 %\n",
      "  validation accuracy:\t\t98.87 %\n",
      "Epoch 56 of 100 took 26.337s\n",
      "  training loss (in-iteration):\t\t0.007446\n",
      "  train accuracy:\t\t99.87 %\n",
      "  validation accuracy:\t\t98.85 %\n",
      "Epoch 57 of 100 took 25.330s\n",
      "  training loss (in-iteration):\t\t0.004475\n",
      "  train accuracy:\t\t99.96 %\n",
      "  validation accuracy:\t\t98.87 %\n",
      "Epoch 58 of 100 took 26.059s\n",
      "  training loss (in-iteration):\t\t0.008173\n",
      "  train accuracy:\t\t99.86 %\n",
      "  validation accuracy:\t\t98.92 %\n",
      "Epoch 59 of 100 took 24.914s\n",
      "  training loss (in-iteration):\t\t0.008241\n",
      "  train accuracy:\t\t99.86 %\n",
      "  validation accuracy:\t\t98.79 %\n",
      "Epoch 60 of 100 took 25.864s\n",
      "  training loss (in-iteration):\t\t0.004406\n",
      "  train accuracy:\t\t99.97 %\n",
      "  validation accuracy:\t\t98.88 %\n",
      "Epoch 61 of 100 took 25.371s\n",
      "  training loss (in-iteration):\t\t0.004423\n",
      "  train accuracy:\t\t99.96 %\n",
      "  validation accuracy:\t\t98.86 %\n",
      "Epoch 62 of 100 took 25.301s\n",
      "  training loss (in-iteration):\t\t0.003312\n",
      "  train accuracy:\t\t99.99 %\n",
      "  validation accuracy:\t\t98.86 %\n",
      "Epoch 63 of 100 took 24.993s\n",
      "  training loss (in-iteration):\t\t0.003232\n",
      "  train accuracy:\t\t99.99 %\n",
      "  validation accuracy:\t\t98.86 %\n",
      "Epoch 64 of 100 took 25.772s\n",
      "  training loss (in-iteration):\t\t0.005435\n",
      "  train accuracy:\t\t99.93 %\n",
      "  validation accuracy:\t\t98.81 %\n",
      "Epoch 65 of 100 took 25.349s\n",
      "  training loss (in-iteration):\t\t0.012924\n",
      "  train accuracy:\t\t99.74 %\n",
      "  validation accuracy:\t\t98.76 %\n",
      "Epoch 66 of 100 took 25.789s\n",
      "  training loss (in-iteration):\t\t0.010351\n",
      "  train accuracy:\t\t99.77 %\n",
      "  validation accuracy:\t\t98.82 %\n",
      "Epoch 67 of 100 took 25.360s\n",
      "  training loss (in-iteration):\t\t0.007376\n",
      "  train accuracy:\t\t99.86 %\n",
      "  validation accuracy:\t\t98.84 %\n",
      "Epoch 68 of 100 took 25.285s\n",
      "  training loss (in-iteration):\t\t0.006522\n",
      "  train accuracy:\t\t99.89 %\n",
      "  validation accuracy:\t\t98.92 %\n",
      "Epoch 69 of 100 took 25.600s\n",
      "  training loss (in-iteration):\t\t0.007505\n",
      "  train accuracy:\t\t99.87 %\n",
      "  validation accuracy:\t\t98.86 %\n",
      "Epoch 70 of 100 took 25.582s\n",
      "  training loss (in-iteration):\t\t0.003898\n",
      "  train accuracy:\t\t99.98 %\n",
      "  validation accuracy:\t\t98.84 %\n",
      "Epoch 71 of 100 took 25.948s\n",
      "  training loss (in-iteration):\t\t0.005473\n",
      "  train accuracy:\t\t99.92 %\n",
      "  validation accuracy:\t\t98.84 %\n",
      "Epoch 72 of 100 took 25.806s\n",
      "  training loss (in-iteration):\t\t0.006444\n",
      "  train accuracy:\t\t99.91 %\n",
      "  validation accuracy:\t\t98.89 %\n",
      "Epoch 73 of 100 took 26.241s\n",
      "  training loss (in-iteration):\t\t0.003974\n",
      "  train accuracy:\t\t99.97 %\n",
      "  validation accuracy:\t\t98.95 %\n",
      "Epoch 74 of 100 took 25.304s\n",
      "  training loss (in-iteration):\t\t0.008943\n",
      "  train accuracy:\t\t99.83 %\n",
      "  validation accuracy:\t\t98.86 %\n",
      "Epoch 75 of 100 took 25.579s\n",
      "  training loss (in-iteration):\t\t0.007463\n",
      "  train accuracy:\t\t99.86 %\n",
      "  validation accuracy:\t\t98.91 %\n",
      "Epoch 76 of 100 took 25.037s\n",
      "  training loss (in-iteration):\t\t0.004336\n",
      "  train accuracy:\t\t99.97 %\n",
      "  validation accuracy:\t\t98.86 %\n",
      "Epoch 77 of 100 took 26.179s\n",
      "  training loss (in-iteration):\t\t0.006814\n",
      "  train accuracy:\t\t99.90 %\n",
      "  validation accuracy:\t\t98.87 %\n",
      "Epoch 78 of 100 took 24.624s\n",
      "  training loss (in-iteration):\t\t0.006880\n",
      "  train accuracy:\t\t99.89 %\n",
      "  validation accuracy:\t\t98.87 %\n",
      "Epoch 79 of 100 took 25.469s\n",
      "  training loss (in-iteration):\t\t0.004860\n",
      "  train accuracy:\t\t99.95 %\n",
      "  validation accuracy:\t\t98.75 %\n",
      "Epoch 80 of 100 took 25.171s\n",
      "  training loss (in-iteration):\t\t0.005520\n",
      "  train accuracy:\t\t99.92 %\n",
      "  validation accuracy:\t\t98.88 %\n",
      "Epoch 81 of 100 took 25.611s\n",
      "  training loss (in-iteration):\t\t0.006245\n",
      "  train accuracy:\t\t99.91 %\n",
      "  validation accuracy:\t\t98.84 %\n",
      "Epoch 82 of 100 took 25.406s\n",
      "  training loss (in-iteration):\t\t0.003884\n",
      "  train accuracy:\t\t99.97 %\n",
      "  validation accuracy:\t\t98.83 %\n",
      "Epoch 83 of 100 took 25.176s\n",
      "  training loss (in-iteration):\t\t0.003117\n",
      "  train accuracy:\t\t99.98 %\n",
      "  validation accuracy:\t\t98.91 %\n",
      "Epoch 84 of 100 took 25.689s\n",
      "  training loss (in-iteration):\t\t0.005465\n",
      "  train accuracy:\t\t99.93 %\n",
      "  validation accuracy:\t\t98.86 %\n",
      "Epoch 85 of 100 took 25.452s\n",
      "  training loss (in-iteration):\t\t0.011164\n",
      "  train accuracy:\t\t99.76 %\n",
      "  validation accuracy:\t\t98.75 %\n",
      "Epoch 86 of 100 took 25.590s\n",
      "  training loss (in-iteration):\t\t0.006671\n",
      "  train accuracy:\t\t99.89 %\n",
      "  validation accuracy:\t\t98.87 %\n",
      "Epoch 87 of 100 took 25.835s\n",
      "  training loss (in-iteration):\t\t0.004936\n",
      "  train accuracy:\t\t99.94 %\n",
      "  validation accuracy:\t\t98.91 %\n",
      "Epoch 88 of 100 took 25.265s\n",
      "  training loss (in-iteration):\t\t0.008609\n",
      "  train accuracy:\t\t99.86 %\n",
      "  validation accuracy:\t\t98.79 %\n",
      "Epoch 89 of 100 took 25.315s\n",
      "  training loss (in-iteration):\t\t0.004693\n",
      "  train accuracy:\t\t99.95 %\n",
      "  validation accuracy:\t\t98.88 %\n",
      "Epoch 90 of 100 took 24.672s\n",
      "  training loss (in-iteration):\t\t0.004610\n",
      "  train accuracy:\t\t99.94 %\n",
      "  validation accuracy:\t\t98.94 %\n",
      "Epoch 91 of 100 took 25.798s\n",
      "  training loss (in-iteration):\t\t0.010129\n",
      "  train accuracy:\t\t99.79 %\n",
      "  validation accuracy:\t\t98.78 %\n",
      "Epoch 92 of 100 took 25.879s\n",
      "  training loss (in-iteration):\t\t0.005598\n",
      "  train accuracy:\t\t99.91 %\n",
      "  validation accuracy:\t\t98.89 %\n",
      "Epoch 93 of 100 took 25.909s\n",
      "  training loss (in-iteration):\t\t0.003491\n",
      "  train accuracy:\t\t99.98 %\n",
      "  validation accuracy:\t\t98.87 %\n",
      "Epoch 94 of 100 took 25.793s\n",
      "  training loss (in-iteration):\t\t0.005317\n",
      "  train accuracy:\t\t99.93 %\n",
      "  validation accuracy:\t\t98.86 %\n",
      "Epoch 95 of 100 took 25.905s\n",
      "  training loss (in-iteration):\t\t0.005626\n",
      "  train accuracy:\t\t99.92 %\n",
      "  validation accuracy:\t\t98.84 %\n",
      "Epoch 96 of 100 took 25.556s\n",
      "  training loss (in-iteration):\t\t0.005428\n",
      "  train accuracy:\t\t99.92 %\n",
      "  validation accuracy:\t\t98.81 %\n",
      "Epoch 97 of 100 took 25.665s\n",
      "  training loss (in-iteration):\t\t0.007893\n",
      "  train accuracy:\t\t99.87 %\n",
      "  validation accuracy:\t\t98.81 %\n",
      "Epoch 98 of 100 took 25.019s\n",
      "  training loss (in-iteration):\t\t0.004625\n",
      "  train accuracy:\t\t99.95 %\n",
      "  validation accuracy:\t\t98.89 %\n",
      "Epoch 99 of 100 took 25.609s\n",
      "  training loss (in-iteration):\t\t0.004457\n",
      "  train accuracy:\t\t99.95 %\n",
      "  validation accuracy:\t\t98.90 %\n",
      "Epoch 100 of 100 took 26.147s\n",
      "  training loss (in-iteration):\t\t0.004147\n",
      "  train accuracy:\t\t99.97 %\n",
      "  validation accuracy:\t\t98.90 %\n"
     ]
    }
   ],
   "source": [
    "#итерации обучения\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "batch_size = 500\n",
    "\n",
    "train_model(num_epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test accuracy:\t\t99.01 %\n",
      "Achievement unlocked: 80lvl Warlock!\n"
     ]
    }
   ],
   "source": [
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    inputs, targets = batch\n",
    "    acc = accuracy_fun(inputs, targets)\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "    \n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))\n",
    "\n",
    "if test_acc / test_batches * 100 > 99:\n",
    "    print (\"Achievement unlocked: 80lvl Warlock!\")\n",
    "else:\n",
    "    print (\"We need more magic!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Report\n",
    "\n",
    "All creative approaches are highly welcome, but at the very least it would be great to mention\n",
    "* the idea;\n",
    "* brief history of tweaks and improvements;\n",
    "* what is the final architecture and why?\n",
    "* what is the training method and, again, why?\n",
    "* Any regularizations and other techniques applied and their effects;\n",
    "\n",
    "\n",
    "There is no need to write strict mathematical proofs (unless you want to).\n",
    " * \"I tried this, this and this, and the second one turned out to be better. And i just didn't like the name of that one\" - OK, but can be better\n",
    " * \"I have analized these and these articles|sources|blog posts, tried that and that to adapt them to my problem and the conclusions are such and such\" - the ideal one\n",
    " * \"I took that code that demo without understanding it, but i'll never confess that and instead i'll make up some pseudoscientific explaination\" - __not_ok__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hi, my name is `Alexey Syromyatnikov`, and here's my story\n",
    "\n",
    "A long ago in a galaxy far far away, when it was still more than an hour before deadline, i got an idea:\n",
    "\n",
    "##### Сначала\n",
    "я проверил нейросеть, состоящую из одного свёрточного блока (свёртка, пулинг, батчнорм) и полносвязного слоя перед выходным слоем. Почему? Потому что такая сеть проста, но всё ещё похожа на успешные большие свёрточные нейросети типа VGG, Alexnet etc.\n",
    "Такая модель давала 98.4%\n",
    "\n",
    "##### Потом \n",
    "был некоторый ряд экспериментов, где я менял различные параметры сети (краткий пересказ ниже). Я исследовал относительно простые сети, которые обучались быстро (менее минуты на эпоху). Это было нужно затем, чтобы покрутить максимальное число параметров (и потому что это было 9 марта).\n",
    "\n",
    "##### В итоге\n",
    "была выбрана архитектура, которую можно увидеть в вышестоящих клетках. Два свёрточных блока и полносвязный для вывода ответа, первый свёрточный содержит два типа свёрток (3х3 и 5х5). Также была использована аугментация обучающей выборки при генерации батчей (сдвиги и повороты).\n",
    "\n",
    "That, having wasted one day [minutes, hours or days] of my life training, got\n",
    "\n",
    "* accuracy on training: 99.97%\n",
    "* accuracy on validation: 98.90%\n",
    "* accuracy on test: 99.01%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** Лог обучения **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Погрешность измерения качества 0.1%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Один блок (свёртка5(10 каналов)-пулинг2-батчнорм) + полносвязный 50 + дропаут 0.1   \n",
    "\n",
    "Батч 1000\n",
    "\n",
    "98.38% за 20 эпох \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "То же самое, но вместо ReLU LeakyReLU\n",
    "\n",
    "98.31 за 20 эпох"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Поменял дропаут на 0.3\n",
    "\n",
    "98.24 за 40 эпох"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* (свёртка5(10 каналов)-пулинг2-батчнорм)\n",
    "* (свёртка3(15 каналов)-пулинг2-батчнорм) \n",
    "* полносвязный 50\n",
    "* дропаут 0.1   \n",
    "\n",
    "Батч 1000\n",
    "\n",
    "98.26% за 20 эпох \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* (свёртка5(5 каналов)-пулинг2-батчнорм)\n",
    "* (свёртка3(15 каналов)-пулинг2-батчнорм) \n",
    "* полносвязный 50\n",
    "* дропаут 0.1   \n",
    "* полносвязный 10\n",
    "* дропаут 0.1   \n",
    "\n",
    "Батч 1000\n",
    "\n",
    "97.44% за 20 эпох \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* local дропаут 0.1   \n",
    "* (свёртка5(5 каналов)-пулинг2-батчнорм)\n",
    "* local дропаут 0.1   \n",
    "* (свёртка3(15 каналов)-пулинг2-батчнорм) \n",
    "* полносвязный 10\n",
    "* дропаут 0.1   \n",
    "\n",
    "Батч 1000\n",
    "\n",
    "96.11% за 20 эпох \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* (свёртка5(10 каналов)-пулинг2-батчнорм)\n",
    "* (свёртка3(15 каналов)-пулинг2-батчнорм) \n",
    "* полносвязный 50\n",
    "* дропаут 0.1   \n",
    "\n",
    "Батч 1000\n",
    "\n",
    "98.18% за 20 эпох \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "То же самое, но вернул ReLU вместо LeakyReLU\n",
    "\n",
    "98.29% за 20 эпох"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "То же самое, но вычел 0.5 из X\n",
    "\n",
    "98.04% за 20 эпох"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "То же самое, не вычитал 0.5 из X, добавил l2 регуляризацию с весом $10^{-5}$\n",
    "\n",
    "98.13% за 20 эпох"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* (свёртка3(10 каналов)-пулинг2-батчнорм)\n",
    "* (свёртка3(10 каналов)-пулинг2-батчнорм)\n",
    "* (свёртка3(15 каналов)-пулинг2-батчнорм) \n",
    "* полносвязный 50\n",
    "* дропаут 0.1   \n",
    "\n",
    "Батч 1000\n",
    "\n",
    "98.15% за 20 эпох \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* (свёртка3(10 каналов)-пулинг2-батчнорм)\n",
    "* (свёртка3(20 каналов)-пулинг2-батчнорм)\n",
    "* (свёртка3(30 каналов)-пулинг2-батчнорм) \n",
    "* полносвязный 50\n",
    "* дропаут 0.1   \n",
    "\n",
    "Батч 1000\n",
    "\n",
    "98.36% за 20 эпох \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* (свёртка3(10 каналов)-пулинг2-батчнорм)\n",
    "* (свёртка3(20 каналов)-пулинг2-батчнорм)\n",
    "* (свёртка3(30 каналов)-пулинг2-батчнорм) \n",
    "* полносвязный 50\n",
    "* дропаут 0.1   \n",
    "* полносвязный 50\n",
    "* дропаут 0.1   \n",
    "\n",
    "Батч 1000\n",
    "\n",
    "98.20% за 20 эпох \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* (свёртка3(10 каналов)-пулинг2-батчнорм)\n",
    "* (свёртка3(20 каналов)-пулинг2-батчнорм)\n",
    "* (свёртка3(30 каналов)-пулинг2-батчнорм) \n",
    "\n",
    "Батч 1000\n",
    "\n",
    "98.22% за 20 эпох \n",
    "\n",
    "Обучается быстрее и быстрее доходит до насыщения, когда меньше полносвязных слоёв"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* (свёртка3(10 каналов)-пулинг2-батчнорм)\n",
    "* (свёртка3(20 каналов)-пулинг2-батчнорм)\n",
    "* (свёртка3(30 каналов)-пулинг2-батчнорм) \n",
    "* полносвязный 50\n",
    "* дропаут 0.1   \n",
    "\n",
    "Батч 500 + аугментация сдвигом\n",
    "\n",
    "98.62% за 100 эпох "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* (свёртка3(10 каналов)-пулинг2-батчнорм)\n",
    "* (свёртка3(20 каналов)-пулинг2-батчнорм)\n",
    "* (свёртка3(30 каналов)-пулинг2-батчнорм) \n",
    "* полносвязный 50\n",
    "* дропаут 0.1   \n",
    "\n",
    "* Батч 500\n",
    "* аугментация сдвигом\n",
    "* rmsprop вместо adam\n",
    "\n",
    "98.6% за 50 эпох "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* (свёртка3(10 каналов)-пулинг2-батчнорм)\n",
    "* (свёртка3(20 каналов)-пулинг2-батчнорм)\n",
    "* (свёртка3(30 каналов)-пулинг2-батчнорм) \n",
    "* полносвязный 50\n",
    "* дропаут 0.1   \n",
    "\n",
    "\n",
    "* Батч 500 \n",
    "* аугментация сдвигом\n",
    "* l2 $10^{-4}$\n",
    "\n",
    "98.3% за 50 эпох "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* (свёртка3(10 каналов)-пулинг2-батчнорм)\n",
    "* (свёртка3(20 каналов)-пулинг2-батчнорм)\n",
    "* (свёртка3(30 каналов)-пулинг2(stride1)-батчнорм) \n",
    "* полносвязный 50\n",
    "* дропаут 0.1   \n",
    "\n",
    "\n",
    "* Батч 500\n",
    "* аугментация сдвигом\n",
    "\n",
    "98.25% за 50 эпох "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Тут некоторые эксперименты, одна эпоха которых проходит слишком долго, что мне становится лень ждать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Я решил упороться и ~~сделать свой GoogleNet~~ соединить свёртки 3х3 и 5х5 на первом уровне. Так как времени мало, я в итоге стал использовать малое число каналов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Input layer (auxilary)\n",
    "input_layer = lasagne.layers.InputLayer(shape = input_shape,input_var=input_X)\n",
    "\n",
    "# conv block\n",
    "conv3 = lasagne.layers.Conv2DLayer(\n",
    "    input_layer, num_filters=4,\n",
    "    filter_size=(3, 3), nonlinearity=lasagne.nonlinearities.rectify,\n",
    "    pad='same',\n",
    ")\n",
    "conv5 = lasagne.layers.Conv2DLayer(\n",
    "    input_layer, num_filters=4,\n",
    "    filter_size=(5, 5), nonlinearity=lasagne.nonlinearities.rectify,\n",
    "    pad='same',\n",
    ")\n",
    "network = lasagne.layers.ConcatLayer([conv3, conv5])\n",
    "network = lasagne.layers.Pool2DLayer(network, pool_size=(2, 2), ignore_border=False)\n",
    "network = lasagne.layers.BatchNormLayer(network)\n",
    "\n",
    "# conv block\n",
    "network = lasagne.layers.Conv2DLayer(\n",
    "    network, num_filters=8,\n",
    "    filter_size=(3, 3), nonlinearity=lasagne.nonlinearities.rectify,\n",
    "    #pad='same',\n",
    ")\n",
    "network = lasagne.layers.Pool2DLayer(network, pool_size=(2, 2), ignore_border=True)\n",
    "network = lasagne.layers.BatchNormLayer(network)\n",
    "\n",
    "# dense block\n",
    "\n",
    "#network = lasagne.layers.DenseLayer(\n",
    "#network, num_units=50,\n",
    "#   nonlinearity = lasagne.nonlinearities.rectify,\n",
    "#)\n",
    "#network = lasagne.layers.DropoutLayer(network, p=0.1)\n",
    "\n",
    "# output\n",
    "network = lasagne.layers.DenseLayer(\n",
    "    network, num_units=10,\n",
    "    nonlinearity=lasagne.nonlinearities.softmax,\n",
    "    name='output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Вот это вот (что выше), даёт 98.6%. Более того, ошибка на трейне и на валидации примерно одинаковая, то есть кажется, что модель слабо переобучается из-за относительно малого числа параметров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Увеличил число каналов во втором блоке до 16 и получил 98.8%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Увеличил число каналов в первом блоке в свёртке 3х3 до 8 и получил 98.9%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Аугментация теперь и с поворотами\n",
    "* 70 эпох с lr=0.001 и 20 эпох с lr=0.0001\n",
    "\n",
    "Получил 98.85%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Вместо adam использовал rmsprop и получил 98.7%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Вернул adam, уменьшил вес у регуляризации до $10^{-6}$ и получил 98.8%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
